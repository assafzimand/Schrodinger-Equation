# Grid Search Configuration
# Searches over loss weights and batch sizes to find optimal hyperparameters

grid_search:
  # Base configuration (will be overridden by grid parameters)
  base_config: config/train.yaml
  
  # Number of epochs for each run
  epochs: 3000
  
  # Grid parameters to search
  parameters:
    # Initial condition loss weight
    # Based on analysis: 1.0 (paper), 10, 20 (moderate boost)
    weight_initial: [1]
    
    # Boundary condition weight (keep constant)
    weight_boundary: [1]
    
    # PDE residual weight (try boosting to improve time evolution)
    weight_residual: [1]
    
    # Batch size (affects gradient noise and convergence)
    batch_size: [512]
    
    # Learning rate (typical range for Adam: 1e-4 to 1e-2)
    learning_rate: [0.001]
    
    # Scheduler type ('reduce_on_plateau', 'step', 'cosine', or null for no scheduler)
    scheduler_type: [null, reduce_on_plateau]
    
    # Scheduler parameters (only used if scheduler_type is not null)
    # factor: how much to reduce LR (0.5 = half, 0.3 = more aggressive)
    scheduler_factor: [0.5]
    # patience: epochs to wait before reducing (balance between stability and responsiveness)
    scheduler_patience: [100, 200, 50]
    # cooldown: epochs to wait after reduction before resuming monitoring
    scheduler_cooldown: [50, 100]
    # min_lr: don't reduce below this (keep some learning capacity)
    scheduler_min_lr: [1e-7]
  
  # MLflow experiment name for grid search
  experiment_name: schrodinger_grid_search
  
  # Save results summary
  results_file: outputs/grid_search_results.csv
  
  # Early stopping criteria (optional)
  early_stopping:
    enabled: false
    patience: 100
    min_delta: 0.001

