# Example Configuration for Resuming Training from a Checkpoint
# Copy this file and modify as needed

train:
  # Training hyperparameters
  epochs: 2000  # Total epochs to train to
  learning_rate: 0.0001
  batch_size: 512
  
  # Hardware settings
  device: auto  # Options: 'auto', 'cuda', 'cpu'
  
  # Model architecture
  hidden_layers: 5
  hidden_neurons: 100
  activation: tanh
  
  # Optimizer settings
  optimizer: adam
  
  # Learning rate scheduler (optional)
  scheduler:
    type: null  # Options: 'reduce_on_plateau', 'step', 'cosine', or null
    factor: 0.5
    patience: 100
    cooldown: 50
    min_lr: 1.0e-7
    threshold: 1.0e-4
    threshold_mode: rel
  
  # Reproducibility
  seed: 42
  dtype: float32
  deterministic: false
  
  # Experiment tracking
  mlflow_experiment: schrodinger_resumed
  checkpoint_dir: outputs/checkpoints
  
  # Pretrained model loading
  # Option 1: Load from direct checkpoint path
  pretrained_checkpoint: "outputs/checkpoints/best_model.pt"
  
  # Option 2: Load from MLflow run ID (uncomment and set)
  # pretrained_run_id: "your_run_id_here"
  
  # Resume training from checkpoint epoch (true) or start from epoch 1 (false)
  resume_training: true

